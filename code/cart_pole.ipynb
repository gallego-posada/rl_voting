{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow Part 2: Policy Gradient Method\n",
    "This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the CartPole problem. For more information, see this [Medium post](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b).\n",
    "\n",
    "For more Reinforcement Learning algorithms, including DQN and Model-based learning in Tensorflow, see my Github repo, [DeepRL-Agents](https://github.com/awjuliani/DeepRL-Agents). \n",
    "\n",
    "Parts of this tutorial are based on code by [Andrej Karpathy](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5) and [korymath](https://gym.openai.com/evaluations/eval_a0aVJrGSyW892vBM04HQA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CartPole Environment\n",
    "If you don't already have the OpenAI gym installed, use  `pip install gym` to grab it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-08 15:37:21,059] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try running the environment with random actions? How well do we do? (Hint: not so well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for this episode was: 23.0\n",
      "Reward for this episode was: 23.0\n",
      "Reward for this episode was: 23.0\n",
      "Reward for this episode was: 28.0\n",
      "Reward for this episode was: 16.0\n",
      "Reward for this episode was: 13.0\n",
      "Reward for this episode was: 21.0\n",
      "Reward for this episode was: 29.0\n",
      "Reward for this episode was: 15.0\n",
      "Reward for this episode was: 43.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print (\"Reward for this episode was:\",reward_sum)\n",
    "        reward_sum = 0\n",
    "        env.reset()\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the task is to achieve a reward of 200 per episode. For every step the agent keeps the pole in the air, the agent recieves a +1 reward. By randomly choosing actions, our reward for each episode is only a couple dozen. Let's make that better with RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our Neural Network agent\n",
    "This time we will be using a Policy neural network that takes observations, passes them through a single hidden layer, and then produces a probability of choosing a left/right movement. To learn more about this network, see [Andrej Karpathy's blog on Policy Gradient networks](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 10 # number of hidden layer neurons\n",
    "batch_size = 5 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-2 # feel free to play with this to train faster or more stably.\n",
    "gamma = 0.99 # discount factor for reward\n",
    "\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#This defines the network as it goes from taking an observation of the environment to \n",
    "#giving a probability of chosing to the action of moving left or right.\n",
    "observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\") # Placeholders to send the final gradients through when we update.\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage function\n",
    "This function allows us to weigh the rewards our agent recieves. In the context of the Cart-Pole task, we want actions that kept the pole in the air a long time to have a large reward, and actions that contributed to the pole falling to have a decreased or negative reward. We do this by weighing the rewards from the end of the episode, with actions at the end being seen as negative, since they likely contributed to the pole falling, and the episode ending. Likewise, early actions are seen as more positive, since they weren't responsible for the pole falling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    #for t in reversed(xrange(0, r.size)):\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the neural network agent, and have it act in the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 5 : 13.000000.  Total average reward 13.000000.\n",
      "Average reward for episode 10 : 23.000000.  Total average reward 13.100000.\n",
      "Average reward for episode 15 : 19.800000.  Total average reward 13.167000.\n",
      "Average reward for episode 20 : 17.200000.  Total average reward 13.207330.\n",
      "Average reward for episode 25 : 20.400000.  Total average reward 13.279257.\n",
      "Average reward for episode 30 : 24.600000.  Total average reward 13.392464.\n",
      "Average reward for episode 35 : 16.000000.  Total average reward 13.418539.\n",
      "Average reward for episode 40 : 20.200000.  Total average reward 13.486354.\n",
      "Average reward for episode 45 : 17.600000.  Total average reward 13.527491.\n",
      "Average reward for episode 50 : 35.000000.  Total average reward 13.742216.\n",
      "Average reward for episode 55 : 22.200000.  Total average reward 13.826793.\n",
      "Average reward for episode 60 : 18.800000.  Total average reward 13.876526.\n",
      "Average reward for episode 65 : 24.200000.  Total average reward 13.979760.\n",
      "Average reward for episode 70 : 18.400000.  Total average reward 14.023963.\n",
      "Average reward for episode 75 : 23.600000.  Total average reward 14.119723.\n",
      "Average reward for episode 80 : 18.800000.  Total average reward 14.166526.\n",
      "Average reward for episode 85 : 24.600000.  Total average reward 14.270861.\n",
      "Average reward for episode 90 : 21.000000.  Total average reward 14.338152.\n",
      "Average reward for episode 95 : 27.000000.  Total average reward 14.464770.\n",
      "Average reward for episode 100 : 16.600000.  Total average reward 14.486123.\n",
      "Average reward for episode 105 : 26.200000.  Total average reward 14.603262.\n",
      "Average reward for episode 110 : 24.600000.  Total average reward 14.703229.\n",
      "Average reward for episode 115 : 28.400000.  Total average reward 14.840197.\n",
      "Average reward for episode 120 : 20.600000.  Total average reward 14.897795.\n",
      "Average reward for episode 125 : 34.200000.  Total average reward 15.090817.\n",
      "Average reward for episode 130 : 31.200000.  Total average reward 15.251909.\n",
      "Average reward for episode 135 : 18.400000.  Total average reward 15.283389.\n",
      "Average reward for episode 140 : 24.400000.  Total average reward 15.374556.\n",
      "Average reward for episode 145 : 30.400000.  Total average reward 15.524810.\n",
      "Average reward for episode 150 : 23.000000.  Total average reward 15.599562.\n",
      "Average reward for episode 155 : 29.800000.  Total average reward 15.741566.\n",
      "Average reward for episode 160 : 27.200000.  Total average reward 15.856151.\n",
      "Average reward for episode 165 : 32.800000.  Total average reward 16.025589.\n",
      "Average reward for episode 170 : 30.600000.  Total average reward 16.171333.\n",
      "Average reward for episode 175 : 28.200000.  Total average reward 16.291620.\n",
      "Average reward for episode 180 : 38.600000.  Total average reward 16.514704.\n",
      "Average reward for episode 185 : 24.000000.  Total average reward 16.589557.\n",
      "Average reward for episode 190 : 29.400000.  Total average reward 16.717661.\n",
      "Average reward for episode 195 : 26.200000.  Total average reward 16.812484.\n",
      "Average reward for episode 200 : 23.600000.  Total average reward 16.880360.\n",
      "Average reward for episode 205 : 17.400000.  Total average reward 16.885556.\n",
      "Average reward for episode 210 : 33.800000.  Total average reward 17.054700.\n",
      "Average reward for episode 215 : 38.800000.  Total average reward 17.272153.\n",
      "Average reward for episode 220 : 35.800000.  Total average reward 17.457432.\n",
      "Average reward for episode 225 : 50.000000.  Total average reward 17.782858.\n",
      "Average reward for episode 230 : 32.400000.  Total average reward 17.929029.\n",
      "Average reward for episode 235 : 25.600000.  Total average reward 18.005739.\n",
      "Average reward for episode 240 : 23.800000.  Total average reward 18.063681.\n",
      "Average reward for episode 245 : 39.600000.  Total average reward 18.279045.\n",
      "Average reward for episode 250 : 38.600000.  Total average reward 18.482254.\n",
      "Average reward for episode 255 : 32.200000.  Total average reward 18.619432.\n",
      "Average reward for episode 260 : 52.400000.  Total average reward 18.957237.\n",
      "Average reward for episode 265 : 42.600000.  Total average reward 19.193665.\n",
      "Average reward for episode 270 : 43.000000.  Total average reward 19.431728.\n",
      "Average reward for episode 275 : 42.000000.  Total average reward 19.657411.\n",
      "Average reward for episode 280 : 55.400000.  Total average reward 20.014837.\n",
      "Average reward for episode 285 : 34.000000.  Total average reward 20.154688.\n",
      "Average reward for episode 290 : 46.400000.  Total average reward 20.417142.\n",
      "Average reward for episode 295 : 35.800000.  Total average reward 20.570970.\n",
      "Average reward for episode 300 : 36.800000.  Total average reward 20.733260.\n",
      "Average reward for episode 305 : 39.000000.  Total average reward 20.915928.\n",
      "Average reward for episode 310 : 50.200000.  Total average reward 21.208769.\n",
      "Average reward for episode 315 : 33.400000.  Total average reward 21.330681.\n",
      "Average reward for episode 320 : 44.200000.  Total average reward 21.559374.\n",
      "Average reward for episode 325 : 27.400000.  Total average reward 21.617780.\n",
      "Average reward for episode 330 : 44.000000.  Total average reward 21.841603.\n",
      "Average reward for episode 335 : 38.800000.  Total average reward 22.011187.\n",
      "Average reward for episode 340 : 44.000000.  Total average reward 22.231075.\n",
      "Average reward for episode 345 : 42.400000.  Total average reward 22.432764.\n",
      "Average reward for episode 350 : 50.200000.  Total average reward 22.710436.\n",
      "Average reward for episode 355 : 41.000000.  Total average reward 22.893332.\n",
      "Average reward for episode 360 : 38.600000.  Total average reward 23.050399.\n",
      "Average reward for episode 365 : 38.400000.  Total average reward 23.203895.\n",
      "Average reward for episode 370 : 43.800000.  Total average reward 23.409856.\n",
      "Average reward for episode 375 : 32.800000.  Total average reward 23.503757.\n",
      "Average reward for episode 380 : 60.200000.  Total average reward 23.870720.\n",
      "Average reward for episode 385 : 51.600000.  Total average reward 24.148012.\n",
      "Average reward for episode 390 : 83.600000.  Total average reward 24.742532.\n",
      "Average reward for episode 395 : 40.800000.  Total average reward 24.903107.\n",
      "Average reward for episode 400 : 52.400000.  Total average reward 25.178076.\n",
      "Average reward for episode 405 : 40.200000.  Total average reward 25.328295.\n",
      "Average reward for episode 410 : 42.400000.  Total average reward 25.499012.\n",
      "Average reward for episode 415 : 46.800000.  Total average reward 25.712022.\n",
      "Average reward for episode 420 : 46.600000.  Total average reward 25.920902.\n",
      "Average reward for episode 425 : 41.600000.  Total average reward 26.077693.\n",
      "Average reward for episode 430 : 55.000000.  Total average reward 26.366916.\n",
      "Average reward for episode 435 : 43.200000.  Total average reward 26.535247.\n",
      "Average reward for episode 440 : 62.200000.  Total average reward 26.891894.\n",
      "Average reward for episode 445 : 42.000000.  Total average reward 27.042975.\n",
      "Average reward for episode 450 : 56.000000.  Total average reward 27.332546.\n",
      "Average reward for episode 455 : 61.800000.  Total average reward 27.677220.\n",
      "Average reward for episode 460 : 61.400000.  Total average reward 28.014448.\n",
      "Average reward for episode 465 : 65.800000.  Total average reward 28.392303.\n",
      "Average reward for episode 470 : 50.400000.  Total average reward 28.612380.\n",
      "Average reward for episode 475 : 45.800000.  Total average reward 28.784257.\n",
      "Average reward for episode 480 : 47.600000.  Total average reward 28.972414.\n",
      "Average reward for episode 485 : 53.400000.  Total average reward 29.216690.\n",
      "Average reward for episode 490 : 67.400000.  Total average reward 29.598523.\n",
      "Average reward for episode 495 : 51.200000.  Total average reward 29.814538.\n",
      "Average reward for episode 500 : 47.000000.  Total average reward 29.986392.\n",
      "Average reward for episode 505 : 56.200000.  Total average reward 30.248528.\n",
      "Average reward for episode 510 : 44.600000.  Total average reward 30.392043.\n",
      "Average reward for episode 515 : 44.600000.  Total average reward 30.534123.\n",
      "Average reward for episode 520 : 56.400000.  Total average reward 30.792781.\n",
      "Average reward for episode 525 : 49.800000.  Total average reward 30.982854.\n",
      "Average reward for episode 530 : 61.200000.  Total average reward 31.285025.\n",
      "Average reward for episode 535 : 50.200000.  Total average reward 31.474175.\n",
      "Average reward for episode 540 : 47.600000.  Total average reward 31.635433.\n",
      "Average reward for episode 545 : 49.600000.  Total average reward 31.815079.\n",
      "Average reward for episode 550 : 66.600000.  Total average reward 32.162928.\n",
      "Average reward for episode 555 : 46.600000.  Total average reward 32.307299.\n",
      "Average reward for episode 560 : 61.400000.  Total average reward 32.598226.\n",
      "Average reward for episode 565 : 67.400000.  Total average reward 32.946243.\n",
      "Average reward for episode 570 : 51.000000.  Total average reward 33.126781.\n",
      "Average reward for episode 575 : 51.200000.  Total average reward 33.307513.\n",
      "Average reward for episode 580 : 70.800000.  Total average reward 33.682438.\n",
      "Average reward for episode 585 : 74.000000.  Total average reward 34.085614.\n",
      "Average reward for episode 590 : 44.600000.  Total average reward 34.190758.\n",
      "Average reward for episode 595 : 52.400000.  Total average reward 34.372850.\n",
      "Average reward for episode 600 : 74.200000.  Total average reward 34.771121.\n",
      "Average reward for episode 605 : 60.400000.  Total average reward 35.027410.\n",
      "Average reward for episode 610 : 82.400000.  Total average reward 35.501136.\n",
      "Average reward for episode 615 : 94.400000.  Total average reward 36.090125.\n",
      "Average reward for episode 620 : 73.200000.  Total average reward 36.461224.\n",
      "Average reward for episode 625 : 97.800000.  Total average reward 37.074611.\n",
      "Average reward for episode 630 : 60.000000.  Total average reward 37.303865.\n",
      "Average reward for episode 635 : 63.600000.  Total average reward 37.566827.\n",
      "Average reward for episode 640 : 111.200000.  Total average reward 38.303158.\n",
      "Average reward for episode 645 : 56.800000.  Total average reward 38.488127.\n",
      "Average reward for episode 650 : 82.800000.  Total average reward 38.931245.\n",
      "Average reward for episode 655 : 88.600000.  Total average reward 39.427933.\n",
      "Average reward for episode 660 : 70.000000.  Total average reward 39.733654.\n",
      "Average reward for episode 665 : 70.200000.  Total average reward 40.038317.\n",
      "Average reward for episode 670 : 119.200000.  Total average reward 40.829934.\n",
      "Average reward for episode 675 : 100.200000.  Total average reward 41.423635.\n",
      "Average reward for episode 680 : 83.400000.  Total average reward 41.843398.\n",
      "Average reward for episode 685 : 103.000000.  Total average reward 42.454964.\n",
      "Average reward for episode 690 : 74.600000.  Total average reward 42.776415.\n",
      "Average reward for episode 695 : 60.800000.  Total average reward 42.956650.\n",
      "Average reward for episode 700 : 102.800000.  Total average reward 43.555084.\n",
      "Average reward for episode 705 : 84.400000.  Total average reward 43.963533.\n",
      "Average reward for episode 710 : 116.200000.  Total average reward 44.685898.\n",
      "Average reward for episode 715 : 121.800000.  Total average reward 45.457039.\n",
      "Average reward for episode 720 : 125.600000.  Total average reward 46.258468.\n",
      "Average reward for episode 725 : 65.000000.  Total average reward 46.445884.\n",
      "Average reward for episode 730 : 85.200000.  Total average reward 46.833425.\n",
      "Average reward for episode 735 : 75.800000.  Total average reward 47.123091.\n",
      "Average reward for episode 740 : 140.600000.  Total average reward 48.057860.\n",
      "Average reward for episode 745 : 139.000000.  Total average reward 48.967281.\n",
      "Average reward for episode 750 : 103.200000.  Total average reward 49.509608.\n",
      "Average reward for episode 755 : 125.600000.  Total average reward 50.270512.\n",
      "Average reward for episode 760 : 93.800000.  Total average reward 50.705807.\n",
      "Average reward for episode 765 : 100.600000.  Total average reward 51.204749.\n",
      "Average reward for episode 770 : 138.400000.  Total average reward 52.076702.\n",
      "Average reward for episode 775 : 136.600000.  Total average reward 52.921935.\n",
      "Average reward for episode 780 : 150.000000.  Total average reward 53.892715.\n",
      "Average reward for episode 785 : 99.200000.  Total average reward 54.345788.\n",
      "Average reward for episode 790 : 128.000000.  Total average reward 55.082330.\n",
      "Average reward for episode 795 : 162.200000.  Total average reward 56.153507.\n",
      "Average reward for episode 800 : 103.400000.  Total average reward 56.625972.\n",
      "Average reward for episode 805 : 85.800000.  Total average reward 56.917712.\n",
      "Average reward for episode 810 : 85.000000.  Total average reward 57.198535.\n",
      "Average reward for episode 815 : 151.000000.  Total average reward 58.136550.\n",
      "Average reward for episode 820 : 148.800000.  Total average reward 59.043184.\n",
      "Average reward for episode 825 : 167.000000.  Total average reward 60.122752.\n",
      "Average reward for episode 830 : 129.600000.  Total average reward 60.817525.\n",
      "Average reward for episode 835 : 128.600000.  Total average reward 61.495350.\n",
      "Average reward for episode 840 : 153.200000.  Total average reward 62.412396.\n",
      "Average reward for episode 845 : 143.000000.  Total average reward 63.218272.\n",
      "Average reward for episode 850 : 136.600000.  Total average reward 63.952089.\n",
      "Average reward for episode 855 : 139.000000.  Total average reward 64.702568.\n",
      "Average reward for episode 860 : 148.600000.  Total average reward 65.541543.\n",
      "Average reward for episode 865 : 156.000000.  Total average reward 66.446127.\n",
      "Average reward for episode 870 : 156.800000.  Total average reward 67.349666.\n",
      "Average reward for episode 875 : 175.400000.  Total average reward 68.430169.\n",
      "Average reward for episode 880 : 157.200000.  Total average reward 69.317868.\n",
      "Average reward for episode 885 : 136.600000.  Total average reward 69.990689.\n",
      "Average reward for episode 890 : 175.400000.  Total average reward 71.044782.\n",
      "Average reward for episode 895 : 157.800000.  Total average reward 71.912334.\n",
      "Average reward for episode 900 : 110.200000.  Total average reward 72.295211.\n",
      "Average reward for episode 905 : 182.000000.  Total average reward 73.392259.\n",
      "Average reward for episode 910 : 183.800000.  Total average reward 74.496336.\n",
      "Average reward for episode 915 : 180.000000.  Total average reward 75.551373.\n",
      "Average reward for episode 920 : 134.200000.  Total average reward 76.137859.\n",
      "Average reward for episode 925 : 200.000000.  Total average reward 77.376481.\n",
      "Average reward for episode 930 : 161.000000.  Total average reward 78.212716.\n",
      "Average reward for episode 935 : 181.800000.  Total average reward 79.248589.\n",
      "Average reward for episode 940 : 152.200000.  Total average reward 79.978103.\n",
      "Average reward for episode 945 : 162.600000.  Total average reward 80.804322.\n",
      "Average reward for episode 950 : 147.400000.  Total average reward 81.470279.\n",
      "Average reward for episode 955 : 191.000000.  Total average reward 82.565576.\n",
      "Average reward for episode 960 : 160.800000.  Total average reward 83.347920.\n",
      "Average reward for episode 965 : 138.000000.  Total average reward 83.894441.\n",
      "Average reward for episode 970 : 155.200000.  Total average reward 84.607496.\n",
      "Average reward for episode 975 : 174.200000.  Total average reward 85.503421.\n",
      "Average reward for episode 980 : 200.000000.  Total average reward 86.648387.\n",
      "Average reward for episode 985 : 199.200000.  Total average reward 87.773903.\n",
      "Average reward for episode 990 : 182.600000.  Total average reward 88.722164.\n",
      "Average reward for episode 995 : 194.000000.  Total average reward 89.774943.\n",
      "Average reward for episode 1000 : 193.400000.  Total average reward 90.811193.\n",
      "Average reward for episode 1005 : 185.600000.  Total average reward 91.759081.\n",
      "Average reward for episode 1010 : 186.400000.  Total average reward 92.705490.\n",
      "Average reward for episode 1015 : 186.400000.  Total average reward 93.642436.\n",
      "Average reward for episode 1020 : 200.000000.  Total average reward 94.706011.\n",
      "Average reward for episode 1025 : 194.800000.  Total average reward 95.706951.\n",
      "Average reward for episode 1030 : 192.200000.  Total average reward 96.671882.\n",
      "Average reward for episode 1035 : 184.400000.  Total average reward 97.549163.\n",
      "Average reward for episode 1040 : 173.200000.  Total average reward 98.305671.\n",
      "Average reward for episode 1045 : 177.600000.  Total average reward 99.098614.\n",
      "Average reward for episode 1050 : 192.800000.  Total average reward 100.035628.\n",
      "Average reward for episode 1055 : 175.000000.  Total average reward 100.785272.\n",
      "Average reward for episode 1060 : 169.800000.  Total average reward 101.475419.\n",
      "Average reward for episode 1065 : 200.000000.  Total average reward 102.460665.\n",
      "Average reward for episode 1070 : 182.800000.  Total average reward 103.264058.\n",
      "Average reward for episode 1075 : 190.200000.  Total average reward 104.133418.\n",
      "Average reward for episode 1080 : 200.000000.  Total average reward 105.092084.\n",
      "Average reward for episode 1085 : 200.000000.  Total average reward 106.041163.\n",
      "Average reward for episode 1090 : 193.200000.  Total average reward 106.912751.\n",
      "Average reward for episode 1095 : 193.600000.  Total average reward 107.779624.\n",
      "Average reward for episode 1100 : 200.000000.  Total average reward 108.701827.\n",
      "Average reward for episode 1105 : 181.600000.  Total average reward 109.430809.\n",
      "Average reward for episode 1110 : 200.000000.  Total average reward 110.336501.\n",
      "Average reward for episode 1115 : 199.800000.  Total average reward 111.231136.\n",
      "Average reward for episode 1120 : 200.000000.  Total average reward 112.118825.\n",
      "Average reward for episode 1125 : 180.800000.  Total average reward 112.805636.\n",
      "Average reward for episode 1130 : 166.400000.  Total average reward 113.341580.\n",
      "Average reward for episode 1135 : 200.000000.  Total average reward 114.208164.\n",
      "Average reward for episode 1140 : 200.000000.  Total average reward 115.066083.\n",
      "Average reward for episode 1145 : 188.800000.  Total average reward 115.803422.\n",
      "Average reward for episode 1150 : 162.400000.  Total average reward 116.269388.\n",
      "Average reward for episode 1155 : 200.000000.  Total average reward 117.106694.\n",
      "Average reward for episode 1160 : 193.000000.  Total average reward 117.865627.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-93ddf314cb0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# so let's only look at it once our agent is doing a good job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrendering\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgalle29/anaconda3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgalle29/anaconda3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgalle29/anaconda3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgalle29/anaconda3/lib/python3.5/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jgalle29/anaconda3/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgalle29/anaconda3/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         '''\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgalle29/anaconda3/lib/python3.5/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "#init = tf.initialize_all_variables()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "        if reward_sum/batch_size > 100 or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            tfp = tfps\n",
    "            xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Average reward for episode',episode_number,': %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "                \n",
    "                if reward_sum/batch_size > 200: \n",
    "                    print (\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "        \n",
    "print (episode_number,'Episodes completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network not only does much better than random actions, but achieves the goal of 200 points per episode, thus solving the task!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
