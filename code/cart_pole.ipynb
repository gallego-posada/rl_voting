{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow Part 2: Policy Gradient Method\n",
    "This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the CartPole problem. For more information, see this [Medium post](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b).\n",
    "\n",
    "For more Reinforcement Learning algorithms, including DQN and Model-based learning in Tensorflow, see my Github repo, [DeepRL-Agents](https://github.com/awjuliani/DeepRL-Agents). \n",
    "\n",
    "Parts of this tutorial are based on code by [Andrej Karpathy](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5) and [korymath](https://gym.openai.com/evaluations/eval_a0aVJrGSyW892vBM04HQA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CartPole Environment\n",
    "If you don't already have the OpenAI gym installed, use  `pip install gym` to grab it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-11 14:11:39,237] Making new env: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try running the environment with random actions? How well do we do? (Hint: not so well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for this episode was: 11.0\n",
      "Reward for this episode was: 21.0\n",
      "Reward for this episode was: 13.0\n",
      "Reward for this episode was: 16.0\n",
      "Reward for this episode was: 19.0\n",
      "Reward for this episode was: 16.0\n",
      "Reward for this episode was: 16.0\n",
      "Reward for this episode was: 14.0\n",
      "Reward for this episode was: 43.0\n",
      "Reward for this episode was: 14.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print (\"Reward for this episode was:\",reward_sum)\n",
    "        reward_sum = 0\n",
    "        env.reset()\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the task is to achieve a reward of 200 per episode. For every step the agent keeps the pole in the air, the agent recieves a +1 reward. By randomly choosing actions, our reward for each episode is only a couple dozen. Let's make that better with RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our Neural Network agent\n",
    "This time we will be using a Policy neural network that takes observations, passes them through a single hidden layer, and then produces a probability of choosing a left/right movement. To learn more about this network, see [Andrej Karpathy's blog on Policy Gradient networks](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 10 # number of hidden layer neurons\n",
    "batch_size = 5 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-2 # feel free to play with this to train faster or more stably.\n",
    "gamma = 0.99 # discount factor for reward\n",
    "nb_layers=2; #BE SURE TO UPDATE THIS VARIABLE FOR CORRECT SAVE \n",
    "\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 5}\n"
     ]
    }
   ],
   "source": [
    "A=[5,3,4]\n",
    "def array_to(arr):\n",
    "    s=str(arr)\n",
    "    s=s.replace('[',' ')\n",
    "    s=s.replace(']',' ')\n",
    "    s=s.replace(' ','')\n",
    "    s=s.replace(',','_')\n",
    "    return s\n",
    "array_to(A)\n",
    "B={}\n",
    "B['test']=5\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/H_4_10_10_1/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-11 18:29:02,315] Restoring parameters from models/H_4_10_10_1/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 5 : 214.000000.  Total average reward 214.000000.\n",
      "Task solved in 5 episodes!\n",
      "[[ 0.54381543  0.03872322  0.51595801  0.21302424 -0.11190394 -0.53458869\n",
      "   0.65915322 -0.24182701  0.62814724 -0.14884129]\n",
      " [ 0.49174616  0.23888563 -0.44936427  0.2078806   0.37950397 -0.07419132\n",
      "   0.58289623  0.47441447  0.05068268  0.02965043]\n",
      " [-0.94739473  0.58078051  0.20058528  0.13413151  0.11316664  0.66154718\n",
      "   0.28853607  0.9173215  -0.2145074   0.00837148]\n",
      " [-0.39093953  0.52243435  0.32740548 -0.68446898  0.31282112  0.93070328\n",
      "   0.30169961  0.42239973 -1.14487195  0.06236639]]\n",
      "5 Episodes completed.\n"
     ]
    }
   ],
   "source": [
    "class Net(object):\n",
    "    def __init__(self, size_layers,layer_function,batch_size=5,learning_rate = 1e-2,gamma = 0.99  ):\n",
    "        self.size_layers=size_layers\n",
    "        self.layer_function=layer_function\n",
    "        self.batch_size=batch_size\n",
    "        self.learning_rate=learning_rate\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        self.Ws=[]\n",
    "        tf.reset_default_graph()\n",
    "        #This defines the network as it goes from taking an observation of the environment to \n",
    "        #giving a probability of chosing to the action of moving left or right.\n",
    "        \n",
    "        size_layers.append(1)\n",
    "        self.observations = tf.placeholder(tf.float32, [None,size_layers[0]] , name=\"input_x\")\n",
    "\n",
    "        Wprev = tf.get_variable(\"W1\", shape=[size_layers[0], size_layers[1]],\n",
    "                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.Ws.append(Wprev)\n",
    "        layerprev = layer_function[0](tf.matmul(self.observations,Wprev))\n",
    "\n",
    "        for i in range(1,len(size_layers)-1):\n",
    "            var_name= 'W'+str(i+1)\n",
    "            Wi =  tf.get_variable(var_name, shape=[size_layers[i], size_layers[i+1]],\n",
    "                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.Ws.append(Wi)\n",
    "            layeri = layer_function[i](tf.matmul(layerprev,Wi))\n",
    "\n",
    "            Wprev=Wi\n",
    "            layerprev=layeri\n",
    "\n",
    "        self.probability = layeri\n",
    "\n",
    "        #From here we define the parts of the network needed for learning a good policy.\n",
    "        self.tvars = tf.trainable_variables()\n",
    "        self.input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "        self.advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "        # The loss function. This sends the weights in the direction of making actions \n",
    "        # that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "        loglik = tf.log(self.input_y*(self.input_y - self.probability) + (1 - self.input_y)*(self.input_y + self.probability))\n",
    "        loss = -tf.reduce_mean(loglik * self.advantages) \n",
    "        self.newGrads = tf.gradients(loss,self.tvars)\n",
    "\n",
    "        # Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "        # We don't just apply gradients after every episode in order to account for noise in the reward signal.\n",
    "        adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "\n",
    "        self.batchGrad=[]\n",
    "        for j in range(1,len(self.size_layers)):\n",
    "            varname='batch_grad'+str(j)\n",
    "            self.batchGrad.append(tf.placeholder(tf.float32,name=varname))# Placeholders to send the final gradients through when we update.\n",
    "        self.updateGrads = adam.apply_gradients(zip(self.batchGrad,self.tvars))\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "    def array_to_str(self,arr):\n",
    "        s=str(arr)\n",
    "        s=s.replace('[',' ')\n",
    "        s=s.replace(']',' ')\n",
    "        s=s.replace(' ','')\n",
    "        s=s.replace(',','_')\n",
    "        return s\n",
    "    #stop_reward: stop criteria based on average reward \n",
    "    def train_network(self,total_episodes,stop_reward,loadModel,saveModel):\n",
    "        xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "        running_reward = None\n",
    "        reward_sum = 0\n",
    "        episode_number = 1\n",
    "        \n",
    "        folder_name='models/'+'H_'+self.array_to_str(self.size_layers)+'/'\n",
    "        model_name = folder_name+'model'\n",
    "        name = model_name+'.meta'\n",
    "        fullrendering = False\n",
    "        rendering=False\n",
    "\n",
    "        # Launch the graph\n",
    "        with self.sess as sess:\n",
    "            sess.run(self.init)\n",
    "            if loadModel:\n",
    "                self.saver.restore(sess, model_name)\n",
    "            observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "            # Reset the gradient placeholder. We will collect gradients in \n",
    "            # gradBuffer until we are ready to update our policy network. \n",
    "            gradBuffer = sess.run(self.tvars)\n",
    "            for ix,grad in enumerate(gradBuffer):\n",
    "                gradBuffer[ix] = grad * 0\n",
    "            while episode_number <= total_episodes:\n",
    "\n",
    "                # Rendering the environment slows things down, \n",
    "                # so let's only look at it once our agent is doing a good job.\n",
    "                if (reward_sum/batch_size > 100 or fullrendering == True) and rendering : \n",
    "                    env.render()\n",
    "                    rendering = True\n",
    "\n",
    "                # Make sure the observation is in a shape the network can handle.\n",
    "                x = np.reshape(observation,[1,D])\n",
    "\n",
    "                # Run the policy network and get an action to take. \n",
    "                tfprob = sess.run(self.probability,feed_dict={self.observations: x})\n",
    "                action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "                xs.append(x) # observation\n",
    "                y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "                ys.append(y)\n",
    "\n",
    "                # step the environment and get new measurements\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                reward_sum += reward\n",
    "\n",
    "                drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "                if done: \n",
    "                    episode_number += 1\n",
    "                    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "                    epx = np.vstack(xs)\n",
    "                    epy = np.vstack(ys)\n",
    "                    epr = np.vstack(drs)\n",
    "                    tfp = tfps\n",
    "                    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "                    # compute the discounted reward backwards through time\n",
    "                    discounted_epr = discount_rewards(epr)\n",
    "                    # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "                    discounted_epr -= np.mean(discounted_epr)\n",
    "                    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "                    # Get the gradient for this episode, and save it in the gradBuffer\n",
    "                    tGrad = sess.run(self.newGrads,feed_dict={self.observations: epx, self.input_y: epy, self.advantages: discounted_epr})\n",
    "                    for ix,grad in enumerate(tGrad):\n",
    "                        gradBuffer[ix] += grad\n",
    "\n",
    "                    # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "                    if episode_number % batch_size == 0: \n",
    "                        feed_dict={}\n",
    "                        for my_index in range(0,len(self.size_layers)-1):\n",
    "                            feed_dict[self.batchGrad[my_index]]=gradBuffer[my_index]\n",
    "                        sess.run(self.updateGrads,feed_dict)\n",
    "                        for ix,grad in enumerate(gradBuffer):\n",
    "                            gradBuffer[ix] = grad * 0\n",
    "\n",
    "                        # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                        print('Average reward for episode',episode_number,': %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "\n",
    "                        if reward_sum/batch_size > stop_reward: \n",
    "                            print (\"Task solved in\",episode_number,'episodes!')\n",
    "                            break\n",
    "\n",
    "                        reward_sum = 0\n",
    "\n",
    "                    observation = env.reset()\n",
    "            #save model\n",
    "            if saveModel:\n",
    "                if not os.path.exists(folder_name):\n",
    "                    os.makedirs(folder_name)\n",
    "                self.saver.save(sess, model_name)\n",
    "                with tf.variable_scope(\"\",reuse=True):\n",
    "                    print(tf.get_variable(\"W1\").eval())\n",
    "\n",
    "        print (episode_number,'Episodes completed.')\n",
    "\n",
    "#test_network = Net([4,10],[tf.nn.relu,tf.nn.sigmoid])\n",
    "test_network = Net([4,10,10],[tf.nn.relu,tf.nn.relu,tf.nn.sigmoid])\n",
    "\n",
    "#test_network.train_network(number_episods,max_score,Load,Save)\n",
    "test_network.train_network(1000,200,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# size_layers: array containing the size of the layers [], first value should be 4 (size of input !!)\n",
    "# layer_function: function apply the the output of each layer (the size of this array should be size (size_hlayers) + 1)\n",
    "# batch_size: every how many episodes to do a param update?\n",
    "# learning_rate: feel free to play with this to train faster or more stably.\n",
    "# gamma: discount factor for reward\n",
    "# D:input dimensionality\n",
    "\n",
    "#example= init_network([4,10],[tf.nn.relu,tf.nn.sigmoid] )\n",
    "def init_network(size_layers,layer_function,batch_size=5,learning_rate = 1e-2,gamma = 0.99  ):\n",
    "    tf.reset_default_graph()\n",
    "    #This defines the network as it goes from taking an observation of the environment to \n",
    "    #giving a probability of chosing to the action of moving left or right.\n",
    "    \n",
    "    size_layers.append(1)\n",
    "    observations = tf.placeholder(tf.float32, [None,size_layers[0]] , name=\"input_x\")\n",
    "    \n",
    "    Wprev = tf.get_variable(\"W1\", shape=[size_layers[0], size_layers[1]],\n",
    "               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    layerprev = layer_function[0](tf.matmul(observations,Wprev))\n",
    "    \n",
    "    for i in range(1,len(size_layers)-1):\n",
    "        var_name= 'W'+str(i+1)\n",
    "        Wi =  tf.get_variable(var_name, shape=[size_layers[i], size_layers[i+1]],\n",
    "               initializer=tf.contrib.layers.xavier_initializer())\n",
    "        layeri = layer_function[i](tf.matmul(layerprev,Wi))\n",
    "        \n",
    "        Wprev=Wi\n",
    "        layerprev=layeri\n",
    "        \n",
    "    probability = layeri\n",
    "\n",
    "    #From here we define the parts of the network needed for learning a good policy.\n",
    "    tvars = tf.trainable_variables()\n",
    "    input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "    advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "    # The loss function. This sends the weights in the direction of making actions \n",
    "    # that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "    loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "    loss = -tf.reduce_mean(loglik * advantages) \n",
    "    newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "    # Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "    # We don't just apply gradients after every episode in order to account for noise in the reward signal.\n",
    "    adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "    \n",
    "    batchGrad=[]\n",
    "    for j in range(1,len(size_layers)):\n",
    "        varname='batch_grad'+str(j)\n",
    "        batchGrad.append(tf.placeholder(tf.float32,name=varname))# Placeholders to send the final gradients through when we update.\n",
    "    updateGrads = adam.apply_gradients(zip(batchGrad,tvars))\n",
    "\n",
    "init_network([4,10],[tf.nn.relu,tf.nn.sigmoid] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage function\n",
    "This function allows us to weigh the rewards our agent recieves. In the context of the Cart-Pole task, we want actions that kept the pole in the air a long time to have a large reward, and actions that contributed to the pole falling to have a decreased or negative reward. We do this by weighing the rewards from the end of the episode, with actions at the end being seen as negative, since they likely contributed to the pole falling, and the episode ending. Likewise, early actions are seen as more positive, since they weren't responsible for the pole falling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    #for t in reversed(xrange(0, r.size)):\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the neural network agent, and have it act in the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/H_4_10_1/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-11 17:37:49,102] Restoring parameters from models/H_4_10_1/model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Variable 'W1:0' shape=(4, 10) dtype=float32_ref> cannot be interpreted as a Tensor. (Tensor Tensor(\"W1:0\", shape=(4, 10), dtype=float32_ref) is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    266\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 267\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    268\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2413\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2414\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2492\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"W1:0\", shape=(4, 10), dtype=float32_ref) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-f96b2a9b0626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[1;31m# Reset the gradient placeholder. We will collect gradients in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[1;31m# gradBuffer until we are ready to update our policy network.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mgradBuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtvars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradBuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mgradBuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \"\"\"\n\u001b[1;32m    407\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \"\"\"\n\u001b[1;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \"\"\"\n\u001b[1;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32mC:\\Users\\alexandre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[0;32m--> 274\u001b[0;31m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[1;32m    275\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mValueError\u001b[0m: Fetch argument <tf.Variable 'W1:0' shape=(4, 10) dtype=float32_ref> cannot be interpreted as a Tensor. (Tensor Tensor(\"W1:0\", shape=(4, 10), dtype=float32_ref) is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 1000\n",
    "#init = tf.initialize_all_variables()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "folder_name='models/H_4_10_1/'\n",
    "model_name = folder_name+'model'\n",
    "name = model_name+'.meta'\n",
    "loadModel=True;\n",
    "saveModel=False;\n",
    "fullrendering = False\n",
    "rendering=False\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    if loadModel:\n",
    "        new_saver = tf.train.import_meta_graph(name,clear_devices=True)\n",
    "        #new_saver.restore(sess, model_name)\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(folder_name))\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "        if (reward_sum/batch_size > 100 or fullrendering == True) and rendering : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            tfp = tfps\n",
    "            xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Average reward for episode',episode_number,': %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "                \n",
    "                if reward_sum/batch_size > 200: \n",
    "                    print (\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "    #save model\n",
    "    if saveModel:\n",
    "        saver = tf.train.Saver([W1,W2])\n",
    "        saver.save(sess, model_name)\n",
    "\n",
    "print (episode_number,'Episodes completed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network not only does much better than random actions, but achieves the goal of 200 points per episode, thus solving the task!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
